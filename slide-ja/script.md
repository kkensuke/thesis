Script
===

## memo
- 修論審査では提出した修論をどのように訂正したかについても述べる
    - 下界のスケーリング(1) の計算ミスを修正した
    - 誤差関数の形とコスト関数の勾配の分散の議論を変更した


## Page 1　タイトル
- 寺師研究室　の上曽山健介です。
- 本日は修論審査よろしくお願いします。
- 私の研究テーマは「量子機械学習におけるデータ符号化由来の勾配消失に関する研究」です。
- 近年、量子コンピューターと機械学習の融合分野である量子機械学習が注目されています。
- その最適化の過程における勾配消失問題について解析しました。


## Page 2　目次
- 本日は、まず前提知識として
    - 量子コンピューター
    - 変分量子アルゴリズム
    - その枠組みでの量子機械学習
    - バレンプラトーと呼ばれる勾配消失問題
- について最初に説明します。
- 続いて、
    - 研究概要と研究成果を説明します。


## Page 4　量子コンピューター
- 量子コンピューターで行われる量子計算は量子回路モデルによって記述され、
    - 量子ビットと呼ばれる 2 準位系
    - 量子ビットの状態を変化させる量子ゲート
    - 量子ビットからの情報を読み出す測定操作
- から構成されます。
- 現在実現されている量子コンピューターは、
    - 量子ビット数が数百量子ビット程度で
    - 誤り訂正を実装できないため、ノイズを無視できません。
    - よって、量子回路の深さに限界があります。
    - そのため、Noisy Intermediate-Scale Quantum device, 略して NISQ と呼ばれています。


## Page 5　変分量子アルゴリズム
- NISQ のような量子コンピューターにおいても実行可能なアルゴリズムとして、変分量子アルゴリズムがあります。
- これは、
    - 量子コンピューターと古典コンピューターのハイブリッドなアルゴリズムです。
    - 量子コンピューター上のパラメーター化された量子回路を用いて、
    - パラメーターに依存した量子状態を生成し、オブザーバブルを測定します。
    - それらのオブザーバブルの期待値によってコスト関数を定義する。
    - コスト関数とその勾配は古典コンピューターで計算して、パラメーター更新し、再びそれらを量子回路に入力します。
    - この流れを繰り返すことで、コスト関数を最小化するパラメーターを探索します。
    - 問題をコスト関数の最小化に帰着させることで、さまざまな問題を解くことができる。
    - 例えば、量子化学計算、組合せ最適化、量子機械学習の研究などの広い分野で応用されています。


## Page 6　量子機械学習
- ここでは、変分量子アルゴリズムの枠組みでの量子機械学習について説明します。
- 特に、右の量子回路を用いて教師あり学習における分類問題を解くことを考えます。
- この量子回路では、
    - データを量子状態に符号化する入力回路と
    - それに続いて、パラメーターを入力する学習回路から構成されます。
- この量子回路によって生成された試行関数としての量子状態を用いてあるオブザーバブルを測定し、その期待値を予測ラベルとして使います。教師ラベルと一致させるようにコスト関数を定義します。関数 f は予測ラベルと教師ラベルの間の差を返す誤差関数です。絶対誤差、二乗誤差関数や交差エントロピー誤差関数などが用いられます。


## Page 7　バレンプラトー
- 変分量子アルゴリズムとその枠組みでの量子機械学習について説明しましたが、
- このアルゴリズムにおいてはバレンプラトーと呼ばれる勾配消失問題が起こりうることが知られており、計算量の観点から重要な課題となっています。
- バレンプラトーとは、コスト関数の勾配の平均が 0 で、コスト関数の勾配の分散が量子ビット数に関して指数関数的に小さくなる現象です。
- 例えば、M 回の測定を行うとき、その測定誤差は 1 / √M になりますが
- 勾配消失が起きると、勾配が指数的に 0 にちかい値をとるので、勾配の正負を判別するには、測定誤差も指数関数的に小さくする必要があります。
- よって、指数関数的に多くの測定を行う必要があり、トータルの計算量も指数関数的に増大し、実用的な計算ができなくなります。
- このように、計算量においては、コスト関数の勾配の分散のスケーリングが重要になります。後に述べる研究成果においてもコスト関数の勾配の分散の上界と下界を導出します。
- 左下の図は、バレンプラトーが起きたときのあるパラメーターに関するコスト関数の断面の様子を表しています。量子ビット数を増やすにつれて、最小点の谷の幅が狭くなり、平坦な領域が広がっているイメージです。
- 右下の図は、先行研究で、深い量子回路において、実際に勾配の分散を数値計算した結果がです。量子ビット数に対して指数関数的に小さくなっていることがわかります。
- バレンプラトーの原因としては、学習回路の深さ、オブザーバブルの局所性、ノイズ（より深刻でコスト関数全体が平坦になる）、機械学習におけるデータの入力が挙げられます。


## Page 9　研究概要
- 以上前提知識として説明したように、研究の背景として、
    - 変分量子アルゴリズムによって機械学習を効率化するための研究が盛んになっていますが、
    - その最適化の過程において、バレンプラトーという勾配消失が起きうることが知られています。
    - そして、その原因となりうるデータ入力による影響は詳しく調べられていません。
- そこで、
    - 量子機械学習におけるデータ入力がバレンプラトーを引き起こさないようにすることを目的としました。
- そのために、
    - データ入力がコスト関数の勾配の分散のスケーリングに与える影響の解析しました。
    - 具体的には、（厳密なコスト関数の勾配の分散を求めることは難しいので）コスト関数の勾配の分散の上界と下界を導出しました。
    - また、コスト関数の勾配の分散のスケーリングはは、二乗誤差や交差エントロピーといった誤差関数の具体的な形に依存しないことを数値的に検証しました。


## Page 11　コスト関数の勾配の分散の上界：設定
- まず、研究成果の一つ目である、コスト関数の勾配の分散の上界について説明します。
- このページでは上界の導出における量子回路の設定を説明します。
    - 量子回路は、青の入力回路と赤の学習回路から構成されます。
    - 入力回路の後の量子状態を \rho_i とし、これを入力状態と呼びます。
    - また、学習回路の後の量子状態を \rho_i(\theta) とします。
    - 学習回路は s 量子ビットユニタリのブロックからなり、それが  ξ 個あるとします。
    - なので、全量子ビットは n = s * ξ 個あります。
    - そして、勾配を計算するパラメーターは学習回路の h 番目の s 量子ビットユニタリに含まれるとします。
    - また、この h 番目の s 量子ビットに入る入力状態の縮約を \rho_i^h と記述します。
    - オブザーバブルはこのように定義され、予測ラベル \ell_i はその期待値で、[0,1] の実数をとります。
    - 学習回路のバラメーターは最初にランダムに初期化され、各 s 量子ビットユニタリはユニタリ 2デザインというランダムな性質を持つと仮定します。
    - ここでは、学習回路とオブザーバブルはバレンプラトーを引き起こさないように設定しました。
## Page 12　コスト関数の勾配の分散の上界：結果
- 以上の設定の下、2値分類におけるコスト関数の勾配の分散の上界を導出します。（基本的に多値分類は2値分類の問題に分解できる）
- y_i は教師ラベルで、0 か 1 です。
- コスト関数は、すべてのデータについての教師ラベルと予測ラベルの差の平均として定義されます。
- 先ほどの量子回路を用いたとき、このコスト関数の勾配の分散の上界を次のように与えられます。
- 上界は3つの要素の積で表されます。
    - 1つ目 A_f は、関数の形 f によるもの
    - 2つ目 r_n,s は、オブザーバブルの形によるもの
    - 3つ目 D_HS は、入力状態によるもの
- 不等式の一番右は、D_HS をさらに上から抑えたものです。指数関数的に減衰する項と入力回路の表現力からなります。
- もし入力回路全体がユニタリ 2デザインというランダムな性質を持てば、表現力の値は 0 になります。よって、上界は指数関数的に減衰する項のみになり、バレンプラトーと呼ばれる勾配消失が起きます。
- しかし、入力回路全体がユニタリ 2デザインというランダムな性質を持つという仮定は十分深い量子回路を必要とします。
- ここでは、そのような強い仮定はせず、浅い入力回路から徐々に層数を増やして行った場合の、上界の振る舞いについて、特に、データ入力に依存する D_HS のスケーリングについて解析します。
## Page 13　コスト関数の勾配の分散の上界：具体的な入力回路の構造
- 解析のため、入力回路には、Alternating Layered Ansatz という青いブロックの構造を用いました。（赤は学習回路を表しています。）
- 層数は点線で区切られる部分の数です。
- ここでは、入力回路全体ではなく、青のユニタリのブロックがぞれぞれユニタリ 2デザインというランダムな性質を持つという仮定で D_HS を計算します。
## Page 14　コスト関数の勾配の分散とその上界の比較
- その仮定の下、上界を計算してプロットしたものが右図です。
- 横軸は入力回路の層数 Lで、それを変化させたときの上界の値をプロットしています。それぞれの色は量子ビット(2,4,6,8)数に対応しています。
- 破線は、上界において回路を十分深くしたときの収束する値を表しています。すなわち、入力回路全体がユニタリ 2デザインというランダムな性質を持つときの上界です。左の図においても同様です。
- 一方で、左は実際にアヤメのデータセットと、Rx,Ry,CNOT からなる青のブロック（これは先行研究に倣った構造です。）を用いて、量子回路シミュレーションを行い、各層数・各量子ビット数に対しコスト関数の勾配の分散を計算したものです。
- 確かに、右図は上界として機能していることがわかります。
## Page 15　層数の必要条件
- 前ページでは上界のすべての要素を含めてプロットしましたが、ここでは、上界の中でも特に D_HS のみをプロットしたものを示します。
- すると、D_HS はある直線を下回らないことが図からわかります。
- D_HS が指数関数的に減衰するとバレンプラトーとなるため、少なくとも上界が指数関数的に減衰しないためには、この直線が量子ビット数の多項式よりも大きくなければなりません。
- よって n^-\gamma <= \beta^-L となり、L ~ log{n} となります。
- これが、バレンプラトーを避けるための層数の必要条件です。
- 上界についての研究成果は以上です。




## Page 17　コスト関数の勾配の分散の下界：設定
- 先ほど導出したコスト関数の勾配の分散の上界では、どのような場合にコスト関数の勾配の分散が小さくなるのかを理解できました。
- 一方、ここでは、コスト関数の勾配の分散の下界を計算することで、どのような場合に勾配の分散が大きくなるのかということを見ていきます。
- まずここでは、コスト関数の勾配の分散の下界を導出するための問題設定を説明します。
- 入力回路は青で、学習回路は赤で表しています。
- 学習回路は s 量子ビットユニタリのブロックからなり、それが  ξ 個あるとします。これらは先ほどと同様にパラメーターはランダムに初期化され、ユニタリ 2デザインというランダムな性質を持つと仮定します。
- そのため全量子ビットは n = s * ξ 個あります。
- 入力回路は解析を簡単にするため R_y ゲートのみの L 層の からなります。
- ここでも、2値分類の問題について考えます。
- コスト関数は解析を簡単にするため、絶対誤差を用いることにします。他の誤差関数を用いた場合については、後ほど考察します。
- ここでは、ラベル 0 の入力データセット X と ラベル 1 の入力データセット Z があり、そのサイズの比率は p:q とします。
- 入力データの各成分は、ガウス分布に従うと仮定します。
- そしてその分散の最小値と最大値をそれぞれ σ_min^2 と σ_max^2 とします。
## Page 18　コスト関数の勾配の分散の下界：結果
- 以上の設定のもとで、コスト関数の勾配の分散の下界は次のようになります。
- p,q は 各ラベルに属するデータセットのサイズの比率です。
- \Sigma_x|j,\Sigma_z|j は j 番目の量子ビットに入力するデータの分散の L 層の和です。
- 特にデータセットのサイズの比率が 1:1、すなわち、p = q = 1/2 のとき下界は次のようになります。
- この表式からわかるように、下界は入力するデータの分散の差が大きいとき、下界は大きくなります。
- しかし、層数 L を増やすと、一般に e^-\Sigma_x|j,e^-\Sigma_z|j は指数関数的に小さくなるので、下界は指数関数的に小さくなる
- また、すべての j について e^-\Sigma_x|j = e^-\Sigma_z|j のとき、下界は 0 となり、自明な下界となる。X = Z という極端な例では、コスト関数が定数になるため、勾配は 0、ゆえに勾配の分散も 0 となります。そのため、下界は 0 となり得ます。
## Page 19　コスト関数の勾配の分散の下界：結果
- もう一つの例として、特にデータセットのサイズの比率が 1:0 の場合を考えます。
- これは One class classification と呼ばれる分類問題に対応します。
- このとき、p = 1, q = 0 であり、コスト関数の勾配の分散の下界は次のように簡略化されます。
- これより、s と L\sigma_{\max}^2 が log(n) 程度であれば、
- 下界が量子ビット数に関して多項式的に減少するため、バレンプラトーは起きません。これは、今の場合においてバレンプラトーを避けるための十分条件です。
- 以上が、勾配の分散の下界についての研究成果です。


## Page 22 誤差関数の形とコスト関数の勾配の分散
- 下界のスケーリングの解析においては、簡単のため、絶対誤差のコスト関数を用いましたが、実際には、二乗誤差、交差エントロピー誤差関数が用いられることが多いです。そこで、それらの勾配の分散のスケーリングについて考えます。結論としては、絶対誤差の勾配の分散のスケーリングと同程度になることを数値的に確認しました。
- 絶対誤差、二乗誤差、交差エントロピー誤差関数以下のように定義されます。
- それらの勾配は次のようになります。ここで、絶対誤差と比較しやすいように勾配を変形しました。
- すると、二乗誤差、交差エントロピー誤差の勾配は、絶対誤差の勾配に |\ell_i - y_i| がかかったものになります。
## Page 23　誤差関数の形とコスト関数の勾配の分散
- ここで、唐突ですが、学習回路が十分深いという仮定して見ます。すると、実は、\ell_i の平均 1/2, 分散は量子ビット数 n に関して指数関数的に小さくなります。すなわち、指数的に 1/2 に集中します。
- そこで、すべての i について \ell_i が 1/2 であると近似してみると、|\ell_i - y_i| も 1/2 であると近似できます。
- すると、前ほどの式から二乗誤差、交差エントロピー誤差の勾配は、絶対誤差の勾配の 1倍, 2倍に近似できます。
- よって、二乗誤差、交差エントロピー誤差の勾配の分散は、絶対誤差の勾配の分散の 1倍, 4倍に近似できます。
- これは、回路が十分深い場合は、二乗誤差、交差エントロピー誤差の勾配の分散のスケーリングが絶対誤差の勾配の分散のスケーリングと同程度であることを近似的に示しています。
## Page 24 誤差関数の形とコスト関数の勾配の分散
- 前ページでは、学習回路が十分深いという仮定の下、勾配の分散のスケーリングと同程度であることを近似的に示しました。
- では、学習回路が浅いときはどうなるのかについて数値的に調べてみます。（実は、学習回路が浅いときは、平均は 1/2 であっても、分散はより大きくなります。そのため、\ell_i ~ 1/2 とみなすことは難しくなります。）
- Tensor Product Ansatz の入力回路、Alternating Layered Ansatz の学習回路を用いて、絶対誤差、二乗誤差、交差エントロピー誤差の勾配の分散を数値計算で求めました。
## Page 25 誤差関数の形とコスト関数の勾配の分散
- その勾配の分散の比をプロットしたものが２つの図です。
- 横軸は、学習回路の層数、縦軸は、勾配の分散の比です。
- 左図は、MSE/MAEの勾配の分散の比、右図は、LOG/MAE の勾配の分散の比をプロットしたものです。
- 破線は、先ほどの近似によって求めた比 1, 4 を表しています。
- 学習回路が浅い場合でも、勾配の分散の比は先ほどの近似比に近いことがわかります。
- したがって、先ほど下界の計算で絶対誤差の勾配の分散のスケーリングを求めたときに得られた結果は、二乗誤差、交差エントロピー誤差の勾配の分散のスケーリングにも適用できるのではないかと考えられます。
- 以上、誤差関数の形とコスト関数の勾配の分散の関係についての考察でした。



## Page 27　まとめ
- まとめ
    - 本研究ではレンプラトーに対するデータ入力の影響について研究を行いました。
    - まず、コスト関数の勾配の分散の上界をデータ入力の観点から導出し、数値的にも検証しました
    - そして、データ入力後の状態のエンタングルメントや入力回路の表現能力が大きい\\　→　上界が小さくなり、バレンプラトーにつながることをみました。
    - また、データ入力の層数が$O(\log{n})$なら、上界のスケーリングは指数関数的ではない事を示した。
    - さらに、入力データがガウス分布に従う場合、入力データの分散が勾配の分散の下界において重要
    - 最後に、勾配の分散のスケーリングは、誤差関数の形にほぼ依存しないことを数値的に確認
- 課題
    - より一般的な入力回路において、コスト関数の勾配の分散の下界を調べることが挙げられます。
    - 今回は勾配の大きさという訓練可能性のみ考えましたが、汎化性能の観点からも入力回路について考慮することも挙げられます。
    - 回路の深さが $O(\log{n})$ 程度に浅ければバレンプラトーは避けられると言ったが、ただ単に浅いだけでは古典的にシミュレートできてしまうので、古典計算が計算量的に難しい入力回路になっているかを調べることも重要となります。



## Page 99 予想される質問
- 上界の計算において、入力回路の表現力によって上から抑えられるということだが、これは学習回路の表現力とみなして考えれば、バレンプラトーになることが示せるのではないか？
    - 解析的に入力回路と学習回路の表現力を統一的に扱うことができれば、そのようなことが示せるかもしれません。
- 入力状態がグローバルユニタリ2デザインであるためには、指数関数的に多くの状態が必要になるのではないか？
    - 確かにそうかもしれない。しかし、ローカルユニタリ2デザインであれば、そこまで多くの状態は必要ではないと考えられる。
- 2値分類しか考えていないが、多値分類の場合はどうなるか？
    - 多値分類はそもそも2値分類の拡張して考えることができます。
        - 例えば One-vs-Rest という手法があります。これは、ラベルが M 個ある場合は、着目するラベルとそれを除いた M - 1 個のラベルを 0 と 1 に分類する2値分類問題を M 個作り、それらの結果を統合することで多値分類を行う手法です。
        - もう一つは、One-vs-One という手法があります。これは、ラベルが M 個ある場合は、M C 2 個の2値分類問題を作り、それらの結果を統合することで多値分類を行う手法です。
    - どちらの場合も、計算量の量子ビット数に関するスケーリングは変わりません。
    - softmax の場合は別？
- すべてのデータをコスト関数に入力しているが、ミニバッチで計算すればコスト関数の勾配の分散は大きくなるのではないか？
    - この場合も入力する訓練データがすべて同じ場合は、絶対誤差のコスト関数は定数となるので勾配はゼロになる。なので、入力するデータが近い場合は勾配が小さくなると考えられる。
- データごとに異なるオブザーバブルを用いた場合の上界はどうなるか？
    - 上界の計算において、r_n,s が最も大きいときのオブザーバブルを用いてさらに上から抑えることができます。
- 古典的な計算がむずがしい入力回路にはどんな回路がある？
    - Instantaneous Quantum Polynomial (IQP) circuit。この量子回路によって生成される状態の内積は古典的な計算が難しくなるらしい。
- 汎化性能という観点からも入力回路についてどのように考える？
    - 汎化誤差は、パラメーター付きゲート数 T とデータセット数 N に関して T/√N で減少することが知られています。よって、N が大きいほど、汎化誤差は小さくなります。